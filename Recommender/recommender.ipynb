{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    " \n",
    "- Initial Steps\n",
    "- Baseline (Ratings)\n",
    "- Baseline+ (Ratings + Genre)\n",
    "- Baseline++ (Ratings + Genre + Subgenre)\n",
    "\n",
    "This is the Jupyter Notebook where the thought process behind and implementation of the recommender system is done. Baseline++ is our proposed model, where subgenres of movies are artificially generated by Gaussian Mixture Models using the scripts of the same movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Steps\n",
    "\n",
    "First, it'll be necessary to parse through the MovieLens dataset and extract some dictionaries/mappings that we'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a mapping from movie IDs to the movie name as well as a mapping from the movie IDs to their genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['id', 'name', 'release_date', 'useless', 'useless1', 'useless2', 'Action', 'Adventure', 'Animation',\n",
    "              'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "            'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "item_df = pd.read_csv(\"ml-100k/u.item\", sep='|', encoding='latin-1', names=col, header=None)\n",
    "item_df = item_df.drop(['release_date', 'useless', 'useless1', 'useless2'], axis=1)\n",
    "\n",
    "id_to_name_dict = {}\n",
    "id_to_genre_dict = {}\n",
    "for index, row in item_df.iterrows():\n",
    "    name = row['name']\n",
    "    name = name.split(\"(\")[0].strip()\n",
    "    id_to_name_dict[name] = row['id']\n",
    "    \n",
    "    genre = []\n",
    "    \n",
    "    if row['Action'] == 1:\n",
    "        genre.append(0)\n",
    "    if row['Adventure'] == 1:\n",
    "        genre.append(1)\n",
    "    if row['Animation'] == 1:\n",
    "        genre.append(2)\n",
    "    if row['Children\\'s'] == 1:\n",
    "        genre.append(3)\n",
    "    if row['Comedy'] == 1:\n",
    "        genre.append(4)\n",
    "    if row['Crime'] == 1:\n",
    "        genre.append(5)\n",
    "    if row['Documentary'] == 1:\n",
    "        genre.append(6)\n",
    "    if row['Drama'] == 1:\n",
    "        genre.append(7)\n",
    "    if row['Fantasy'] == 1:\n",
    "        genre.append(8)\n",
    "    if row['Film-Noir'] == 1:\n",
    "        genre.append(9)\n",
    "    if row['Horror'] == 1:\n",
    "        genre.append(10)\n",
    "    if row['Musical'] == 1:\n",
    "        genre.append(11)\n",
    "    if row['Mystery'] == 1:\n",
    "        genre.append(12)\n",
    "    if row['Romance'] == 1:\n",
    "        genre.append(13)\n",
    "    if row['Sci-Fi'] == 1:\n",
    "        genre.append(14)\n",
    "    if row['Thriller'] == 1:\n",
    "        genre.append(15)\n",
    "    if row['War'] == 1:\n",
    "        genre.append(16)\n",
    "    if row['Western'] == 1:\n",
    "        genre.append(17)\n",
    "        \n",
    "    id_to_genre_dict[row['id']] = genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"id_name_dict.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(id_to_name_dict, output_pickle)\n",
    "    \n",
    "with open(\"id_to_genre_dict.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(id_to_genre_dict, output_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a mapping of the genre to the movie IDs of the same genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.drop(['name'], axis=1)\n",
    "genre_ids = [[], [], [], [], [], [], [], [], [], [], [] ,[] ,[] ,[] ,[] ,[] ,[] ,[]]\n",
    "for index, row in item_df.iterrows():\n",
    "    name = row['id']\n",
    "    id_to_name_dict[name] = row['id']\n",
    "    \n",
    "    genre = []\n",
    "    \n",
    "    if row['Action'] == 1:\n",
    "        genre_ids[0].append(name)\n",
    "    if row['Adventure'] == 1:\n",
    "        genre_ids[1].append(name)\n",
    "    if row['Animation'] == 1:\n",
    "        genre_ids[2].append(name)\n",
    "    if row['Children\\'s'] == 1:\n",
    "        genre_ids[3].append(name)\n",
    "    if row['Comedy'] == 1:\n",
    "        genre_ids[4].append(name)\n",
    "    if row['Crime'] == 1:\n",
    "        genre_ids[5].append(name)\n",
    "    if row['Documentary'] == 1:\n",
    "        genre_ids[6].append(name)\n",
    "    if row['Drama'] == 1:\n",
    "        genre_ids[7].append(name)\n",
    "    if row['Fantasy'] == 1:\n",
    "        genre_ids[8].append(name)\n",
    "    if row['Film-Noir'] == 1:\n",
    "        genre_ids[9].append(name)\n",
    "    if row['Horror'] == 1:\n",
    "        genre_ids[10].append(name)\n",
    "    if row['Musical'] == 1:\n",
    "        genre_ids[11].append(name)\n",
    "    if row['Mystery'] == 1:\n",
    "        genre_ids[12].append(name)\n",
    "    if row['Romance'] == 1:\n",
    "        genre_ids[13].append(name)\n",
    "    if row['Sci-Fi'] == 1:\n",
    "        genre_ids[14].append(name)\n",
    "    if row['Thriller'] == 1:\n",
    "        genre_ids[15].append(name)\n",
    "    if row['War'] == 1:\n",
    "        genre_ids[16].append(name)\n",
    "    if row['Western'] == 1:\n",
    "        genre_ids[17].append(name)\n",
    "\n",
    "with open(\"genre_ids.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(genre_ids, output_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "First, let's establish a baseline where we perform collaborative filtering using just cosine similarity of rating of the movies by users. We take our MovieLens dataset and eliminate all of the movies where we don't have scripts (to maintain consistency of the amount of training data between models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_dataset(row):\n",
    "    if row['item_id'] in scripts_id_dict:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "with open(\"scripts_id_content.pickle\", \"rb\") as output_pickle:\n",
    "    scripts_id_dict = pickle.load(output_pickle)\n",
    "    \n",
    "    col = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    df = pd.read_csv(\"ml-100k/u.data\", names=col, sep='\\t', header=None, encoding='latin-1')\n",
    "    df['in_dataset'] = df.apply(lambda row: in_dataset(row), axis=1)\n",
    "    df = df[df['in_dataset'] == True]\n",
    "    df = df.drop(['in_dataset'], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting into an 80/20 train-test split and normalize by subtracting the mean from each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_training = df.sample(frac=0.8)\n",
    "ratings_test = df.drop(ratings_training.index)\n",
    "\n",
    "rating_mean = ratings_training.groupby(['item_id'], as_index = False, sort = False).mean().rename(columns = {'rating': 'rating_mean'})[['item_id','rating_mean']]\n",
    "adjusted_ratings = pd.merge(ratings_training, rating_mean,on = 'item_id', how = 'left', sort = False)\n",
    "adjusted_ratings['rating_adjusted'] = adjusted_ratings['rating'] - adjusted_ratings['rating_mean']\n",
    "#replace 0 adjusted rating values to 1*e-8 in order to avoid 0 denominator\n",
    "adjusted_ratings.loc[adjusted_ratings['rating_adjusted'] == 0, 'rating_adjusted'] = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start building the similarity matrix by iterating through every user and then every movie they've watched and calculating similarity values between the two (to be put in the weight column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w_matrix(adjusted_ratings):\n",
    "    \n",
    "    w_matrix_columns = ['movie_a', 'movie_b', 'weight']\n",
    "    w_matrix = pd.DataFrame(columns=w_matrix_columns)\n",
    "    \n",
    "    distinct_movies = np.unique(adjusted_ratings['item_id'])\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for movie_a in distinct_movies:\n",
    "        \n",
    "        if i%2==0:\n",
    "            print(i , \"out of \", len(distinct_movies))\n",
    "        \n",
    "        user_data = adjusted_ratings[adjusted_ratings['item_id'] == movie_a]\n",
    "        distinct_users = np.unique(user_data['user_id'])\n",
    "        \n",
    "        record_row_columns = ['user_id', 'movie_a', 'movie_b', 'rating_adjusted_a', 'rating_adjusted_b']\n",
    "        record_movie_a_b = pd.DataFrame(columns=record_row_columns)\n",
    "        \n",
    "        for c_user_id in distinct_users:\n",
    "            c_movie_a_rating = user_data[user_data['user_id'] == c_user_id]['rating_adjusted'].iloc[0]\n",
    "            c_user_data = adjusted_ratings[(adjusted_ratings['user_id'] == c_user_id) & (adjusted_ratings['item_id'] != movie_a)]\n",
    "            c_distinct_movies = np.unique(c_user_data['item_id'])\n",
    "            \n",
    "            for movie_b in c_distinct_movies:\n",
    "                c_movie_b_rating = c_user_data[c_user_data['item_id'] == movie_b]['rating_adjusted'].iloc[0]\n",
    "                record_row = pd.Series([c_user_id, movie_a, movie_b, c_movie_a_rating, c_movie_b_rating], index=record_row_columns)\n",
    "                record_movie_a_b = record_movie_a_b.append(record_row, ignore_index=True)\n",
    "                \n",
    "        distinct_movie_b = np.unique(record_movie_a_b['movie_b'])\n",
    "        \n",
    "        for movie_b in distinct_movie_b:\n",
    "            paired_movie_a_b = record_movie_a_b[record_movie_a_b['movie_b'] == movie_b]\n",
    "            \n",
    "            sim_value_numerator = (paired_movie_a_b['rating_adjusted_a'] * paired_movie_a_b['rating_adjusted_b']).sum()\n",
    "            sim_value_denominator = np.sqrt(np.square(paired_movie_a_b['rating_adjusted_a']).sum()) * np.sqrt(np.square(paired_movie_a_b['rating_adjusted_b']).sum())\n",
    "            sim_value_denominator = sim_value_denominator if sim_value_denominator != 0 else 1e-8\n",
    "            sim_value = sim_value_numerator / sim_value_denominator\n",
    "            w_matrix = w_matrix.append(pd.Series([movie_a, movie_b, sim_value], index=w_matrix_columns), ignore_index=True)\n",
    "        i += 1\n",
    "        \n",
    "    with open(\"w_matrix.pickle\", 'wb') as output:\n",
    "        pickle.dump(w_matrix, output, pickle.HIGHEST_PROTOCOL)\n",
    "    output.close()\n",
    "        \n",
    "    return w_matrix\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of  243\n",
      "2 out of  243\n",
      "4 out of  243\n",
      "6 out of  243\n",
      "8 out of  243\n",
      "10 out of  243\n",
      "12 out of  243\n",
      "14 out of  243\n",
      "16 out of  243\n",
      "18 out of  243\n",
      "20 out of  243\n",
      "22 out of  243\n",
      "24 out of  243\n",
      "26 out of  243\n",
      "28 out of  243\n",
      "30 out of  243\n",
      "32 out of  243\n",
      "34 out of  243\n",
      "36 out of  243\n",
      "38 out of  243\n",
      "40 out of  243\n",
      "42 out of  243\n",
      "44 out of  243\n",
      "46 out of  243\n",
      "48 out of  243\n",
      "50 out of  243\n",
      "52 out of  243\n",
      "54 out of  243\n",
      "56 out of  243\n",
      "58 out of  243\n",
      "60 out of  243\n",
      "62 out of  243\n",
      "64 out of  243\n",
      "66 out of  243\n",
      "68 out of  243\n",
      "70 out of  243\n",
      "72 out of  243\n",
      "74 out of  243\n",
      "76 out of  243\n",
      "78 out of  243\n",
      "80 out of  243\n",
      "82 out of  243\n",
      "84 out of  243\n",
      "86 out of  243\n",
      "88 out of  243\n",
      "90 out of  243\n",
      "92 out of  243\n",
      "94 out of  243\n",
      "96 out of  243\n",
      "98 out of  243\n",
      "100 out of  243\n",
      "102 out of  243\n",
      "104 out of  243\n",
      "106 out of  243\n",
      "108 out of  243\n",
      "110 out of  243\n",
      "112 out of  243\n",
      "114 out of  243\n",
      "116 out of  243\n",
      "118 out of  243\n",
      "120 out of  243\n",
      "122 out of  243\n",
      "124 out of  243\n",
      "126 out of  243\n",
      "128 out of  243\n",
      "130 out of  243\n",
      "132 out of  243\n",
      "134 out of  243\n",
      "136 out of  243\n",
      "138 out of  243\n",
      "140 out of  243\n",
      "142 out of  243\n",
      "144 out of  243\n",
      "146 out of  243\n",
      "148 out of  243\n",
      "150 out of  243\n",
      "152 out of  243\n",
      "154 out of  243\n",
      "156 out of  243\n",
      "158 out of  243\n",
      "160 out of  243\n",
      "162 out of  243\n",
      "164 out of  243\n",
      "166 out of  243\n",
      "168 out of  243\n",
      "170 out of  243\n",
      "172 out of  243\n",
      "174 out of  243\n",
      "176 out of  243\n",
      "178 out of  243\n",
      "180 out of  243\n",
      "182 out of  243\n",
      "184 out of  243\n",
      "186 out of  243\n",
      "188 out of  243\n",
      "190 out of  243\n",
      "192 out of  243\n",
      "194 out of  243\n",
      "196 out of  243\n",
      "198 out of  243\n",
      "200 out of  243\n",
      "202 out of  243\n",
      "204 out of  243\n",
      "206 out of  243\n",
      "208 out of  243\n",
      "210 out of  243\n",
      "212 out of  243\n",
      "214 out of  243\n",
      "216 out of  243\n",
      "218 out of  243\n",
      "220 out of  243\n",
      "222 out of  243\n",
      "224 out of  243\n",
      "226 out of  243\n",
      "228 out of  243\n",
      "230 out of  243\n",
      "232 out of  243\n",
      "234 out of  243\n",
      "236 out of  243\n",
      "238 out of  243\n",
      "240 out of  243\n",
      "242 out of  243\n"
     ]
    }
   ],
   "source": [
    "w_matrix = build_w_matrix(adjusted_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create two functions to have our recommender system predict/recommend and be evaluated by converting the ratings to negative and positive and predicting ratings to see if they fall within those two categories. \n",
    "\n",
    "Note: \n",
    "\n",
    "Precision: percentage of all results that are relevant (false positives, recommends movie when it shouldn't, in denominator)\n",
    "Recall: percentage of all relevant results correctly classified (false negatives, doesn't recommend movie when it should, in denominator)\n",
    "\n",
    "We would value precision more in this case since the benefits of making a correct recommendation to a user is much more substantial than the harms of not making a recommendation that it should, especially since a false positive could potentially ward users away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(user_id, item_id, w_matrix, adjusted_ratings, rating_mean):\n",
    "    if rating_mean[rating_mean['item_id'] == item_id].shape[0] > 0:\n",
    "        mean_rating = rating_mean[rating_mean['item_id'] == item_id]['rating_mean'].iloc[0]\n",
    "    else:\n",
    "        mean_rating = 2.5\n",
    "\n",
    "    user_other_ratings = adjusted_ratings[adjusted_ratings['user_id'] == user_id]\n",
    "    user_distinct_movies = np.unique(user_other_ratings['item_id'])\n",
    "    sum_weighted_other_ratings = 0\n",
    "    sum_weights = 0\n",
    "    for movie_j in user_distinct_movies:\n",
    "        if rating_mean[rating_mean['item_id'] == movie_j].shape[0] > 0:\n",
    "            rating_mean_j = rating_mean[rating_mean['item_id'] == movie_j]['rating_mean'].iloc[0]\n",
    "        else:\n",
    "            rating_mean_j = 2.5\n",
    "        w_movie_a_b = w_matrix[(w_matrix['movie_a'] == item_id) & (w_matrix['movie_b'] == movie_j)]\n",
    "        if w_movie_a_b.shape[0] > 0:\n",
    "            user_rating_j = user_other_ratings[user_other_ratings['item_id']==movie_j]\n",
    "            sum_weighted_other_ratings += (user_rating_j['rating'].iloc[0] - rating_mean_j) * w_movie_a_b['weight'].iloc[0]\n",
    "            sum_weights += np.abs(w_movie_a_b['weight'].iloc[0])\n",
    "\n",
    "    # if sum_weights is 0 (which may be because of no ratings from new users), use the mean ratings\n",
    "    if sum_weights == 0:\n",
    "        predicted_rating = mean_rating\n",
    "    # sum_weights is bigger than 0\n",
    "    else:\n",
    "        predicted_rating = mean_rating + sum_weighted_other_ratings/sum_weights\n",
    "\n",
    "    return predicted_rating\n",
    "\n",
    "def binary_eval(ratings_test, w_matrix, adjusted_ratings, rating_mean):\n",
    "    # predict all the ratings for test data\n",
    "    ratings_test = ratings_test.assign(predicted_rating = pd.Series(np.zeros(ratings_test.shape[0])))\n",
    "    curr = 0\n",
    "    for index, row_rating in ratings_test.iterrows():\n",
    "        if curr % 200 == 0:\n",
    "            print(curr , \"out of \", len(ratings_test))\n",
    "        predicted_rating = predict(row_rating['user_id'], row_rating['item_id'], w_matrix, adjusted_ratings, rating_mean)\n",
    "        ratings_test.loc[index, 'predicted_rating'] = predicted_rating\n",
    "        curr += 1\n",
    "    tp = ratings_test.query('(rating >= 2.5) & (predicted_rating >= 2.5)').shape[0]\n",
    "    fp = ratings_test.query('(rating < 2.5) & (predicted_rating >= 2.5)').shape[0]\n",
    "    fn = ratings_test.query('(rating >= 2.5) & (predicted_rating < 2.5)').shape[0]\n",
    "\n",
    "    # calculate the precision and recall\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    return (precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of  7070\n",
      "200 out of  7070\n",
      "400 out of  7070\n",
      "600 out of  7070\n",
      "800 out of  7070\n",
      "1000 out of  7070\n",
      "1200 out of  7070\n",
      "1400 out of  7070\n",
      "1600 out of  7070\n",
      "1800 out of  7070\n",
      "2000 out of  7070\n",
      "2200 out of  7070\n",
      "2400 out of  7070\n",
      "2600 out of  7070\n",
      "2800 out of  7070\n",
      "3000 out of  7070\n",
      "3200 out of  7070\n",
      "3400 out of  7070\n",
      "3600 out of  7070\n",
      "3800 out of  7070\n",
      "4000 out of  7070\n",
      "4200 out of  7070\n",
      "4400 out of  7070\n",
      "4600 out of  7070\n",
      "4800 out of  7070\n",
      "5000 out of  7070\n",
      "5200 out of  7070\n",
      "5400 out of  7070\n",
      "5600 out of  7070\n",
      "5800 out of  7070\n",
      "6000 out of  7070\n",
      "6200 out of  7070\n",
      "6400 out of  7070\n",
      "6600 out of  7070\n",
      "6800 out of  7070\n",
      "7000 out of  7070\n",
      "Evaluation result - precision: 0.877292, recall: 0.988773\n"
     ]
    }
   ],
   "source": [
    "eval_result = binary_eval(ratings_test, w_matrix, adjusted_ratings, rating_mean)\n",
    "print('Evaluation result - precision: %f, recall: %f' % eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline+ \n",
    "\n",
    "We can improve upon these metrics by incorporating the genre of movies into our similarity matrix. For every given similarity value, if the two movies are of the same genre, then the similarity value will be multipled by a number greater than 1 (increased). \n",
    "\n",
    "Note: This factor needs to be fiddled around with to find an optimal value. Future steps include a grid search for an idea value but initial checks seem to indicate that a value between 1 and 2 is ideal. Below we use 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"id_genre_dict.pickle\", \"rb\") as output_pickle:\n",
    "    id_to_genre_dict = pickle.load(output_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_w_matrix_by_genre(w_matrix, genre_dict, factor=1.2):\n",
    "    adjusted_w_matrix = w_matrix.copy()\n",
    "    \n",
    "    for index, row in w_matrix.iterrows():\n",
    "        a_genres = genre_dict[int(row['movie_a'])]\n",
    "        b_genres = genre_dict[int(row['movie_b'])]\n",
    "        if not set(a_genres).isdisjoint(b_genres): #if there is overlap, this is true\n",
    "            adjusted_w_matrix.set_value(index, 'weight', row['weight']*factor)\n",
    "    \n",
    "    return adjusted_w_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "genre_dict = id_to_genre_dict\n",
    "w_matrix_genres = adjust_w_matrix_by_genre(w_matrix, genre_dict, 1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of  7070\n",
      "200 out of  7070\n",
      "400 out of  7070\n",
      "600 out of  7070\n",
      "800 out of  7070\n",
      "1000 out of  7070\n",
      "1200 out of  7070\n",
      "1400 out of  7070\n",
      "1600 out of  7070\n",
      "1800 out of  7070\n",
      "2000 out of  7070\n",
      "2200 out of  7070\n",
      "2400 out of  7070\n",
      "2600 out of  7070\n",
      "2800 out of  7070\n",
      "3000 out of  7070\n",
      "3200 out of  7070\n",
      "3400 out of  7070\n",
      "3600 out of  7070\n",
      "3800 out of  7070\n",
      "4000 out of  7070\n",
      "4200 out of  7070\n",
      "4400 out of  7070\n",
      "4600 out of  7070\n",
      "4800 out of  7070\n",
      "5000 out of  7070\n",
      "5200 out of  7070\n",
      "5400 out of  7070\n",
      "5600 out of  7070\n",
      "5800 out of  7070\n",
      "6000 out of  7070\n",
      "6200 out of  7070\n",
      "6400 out of  7070\n",
      "6600 out of  7070\n",
      "6800 out of  7070\n",
      "7000 out of  7070\n",
      "Evaluation result - precision: 0.881028, recall: 0.986821\n"
     ]
    }
   ],
   "source": [
    "# run the evaluation\n",
    "eval_result = binary_eval(ratings_test, w_matrix_genres, adjusted_ratings, rating_mean)\n",
    "print('Evaluation result - precision: %f, recall: %f' % eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline++\n",
    "\n",
    "Here is where the subgenres are incorporated. The idea is similar to that of Baseline+: if two movies are in the same subgenre, then they should be considered more similar. \n",
    "\n",
    "However, these subgenres are not easily obtained: most movies are just broadly labeled into categories such as \"drama,\" \"comedy,\" or \"horror.\" However, there are many distinctions within each of those categories that would make recommendation based solely on those categorizations faulty.\n",
    "\n",
    "One way to obtain these subgenres is to analyze the scripts of each movie. Each word will be converted into a 300 length vector through a pretrained FastText model. FastText is a model that trains on corpora (the one used was trained on Wikipedia) and then uses an n-gram model to creates word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce time complexity, we'll first create a list of all unique words in the dataset before creating a word-to-vector mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set()\n",
    "with open(\"scripts_new.csv\", \"r\") as f:\n",
    "    scripts = f.readlines()\n",
    "    for single in scripts:\n",
    "        single = single.split(\"|\")[1]\n",
    "        single = cleanhtml(single).lower()\n",
    "        single = single.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        for word in word_tokenize(single):\n",
    "            unique_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", \"w\") as f:\n",
    "    for word in unique_words:\n",
    "        f.write(word + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the vocab.txt file was sent through the FastText bin file on a separate machine due to the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84d20f41a3d4c68a8c603791142bc7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=219941), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_dict = {}\n",
    "\n",
    "with open(\"dict.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        word = line.split(\" \")[0]\n",
    "        vector = line.split(\" \")[1:][:-1]\n",
    "        assert len(vector) == 300\n",
    "        \n",
    "        if word not in vector_dict:\n",
    "            vector_list = []\n",
    "            for num in vector:\n",
    "                vector_list.append(float(num))\n",
    "                \n",
    "            vector_dict[word] = vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f05094819143b887f7fc6919e880db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1177), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scripts_content = {}\n",
    "with open(\"scripts_new.csv\", \"r\") as f:\n",
    "    scripts = f.readlines()\n",
    "    for single_index in tqdm(range(len(scripts))):\n",
    "        single = scripts[single_index]\n",
    "        title = single.split(\"|\")[0]\n",
    "        single = single.split(\"|\")[1]\n",
    "        single = cleanhtml(single).lower()\n",
    "        single = single.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        content = []\n",
    "        \n",
    "        for word in word_tokenize(single):\n",
    "            try:\n",
    "                content.append(vector_dict[word])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if title not in scripts_content:\n",
    "            scripts_content[title] = content\n",
    "\n",
    "with open(\"scripts_vector_dict.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(scripts_content, output_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_scripts_content = {}\n",
    "id_genre_dict = id_to_genre_dict\n",
    "id_name_dict = id_to_name_dict\n",
    "\n",
    "for movie_name in list(scripts_content.keys()):\n",
    "    if movie_name in id_name_dict:\n",
    "        id_scripts_content[id_name_dict[movie_name]] = scripts_content[movie_name]\n",
    "\n",
    "genre_dist = []\n",
    "for movie_id in id_scripts_content:\n",
    "    if movie_id in id_genre_dict:\n",
    "        for label in id_genre_dict[movie_id]:\n",
    "            genre_dist.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the distribution of our scripts, which we use to inform our N value for our Gaussian Mixture Models (another hyperparameter that needs a grid search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([68, 25,  7, 10, 61, 28,  0, 95,  2,  8, 23,  9, 20, 41, 32, 72, 14,\n",
      "        3]), array([ 0.        ,  0.94444444,  1.88888889,  2.83333333,  3.77777778,\n",
      "        4.72222222,  5.66666667,  6.61111111,  7.55555556,  8.5       ,\n",
      "        9.44444444, 10.38888889, 11.33333333, 12.27777778, 13.22222222,\n",
      "       14.16666667, 15.11111111, 16.05555556, 17.        ]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([68., 25.,  7., 10., 61., 28.,  0., 95.,  2.,  8., 23.,  9., 20.,\n",
       "        41., 32., 72., 14.,  3.]),\n",
       " array([ 0.        ,  0.94444444,  1.88888889,  2.83333333,  3.77777778,\n",
       "         4.72222222,  5.66666667,  6.61111111,  7.55555556,  8.5       ,\n",
       "         9.44444444, 10.38888889, 11.33333333, 12.27777778, 13.22222222,\n",
       "        14.16666667, 15.11111111, 16.05555556, 17.        ]),\n",
       " <a list of 18 Patch objects>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.histogram(genre_dist, bins=18))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(genre_dist, bins=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scripts_id_content.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(id_scripts_content, output_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in genre_dict.items():\n",
    "    doublet = [value, {}]\n",
    "    genre_dict[key] = doublet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa35412018fe4cbfb4e43fb554061dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=251), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1642cf20ac6545daba731b524e87caf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=135), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80c9ee13686455b8921eb70d21deeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=42), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf70d08fe5348a99dd7bf7c7b63a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951d45eb13d1427682451d0c81ba51bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=505), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5494bf8745b442ebccbe5837a613eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=109), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725ab0ddf3684fb4838c84137c5dea8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cf8086189f426f8d2da3f7d806348c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=725), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234d4474c4c84f67b5687d5ad7fb3938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e44324e02044c64beba18283f707709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1580469d854e87bd71aa83ada55e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=92), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8496141bbe64e5ebe774d20215f32b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=56), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd33495218d94b38b6c916cb44e0e2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648be9c06d5d40aea52eed46e26becbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bfb3c7c0bf4efaa4693f8a5a6b7ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716697cd6d054377b7b1ca0a9b272b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=251), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b738c763734345a4989c6c935ee4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=71), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150f1a48fada4369ba248b43ea894295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "all_n = [5, 3, 1, 1, 5, 3, 1, 5, 1, 1, 3, 1, 3, 3, 3, 5, 1, 1]\n",
    "\n",
    "for genre in range(18):\n",
    "    \n",
    "    N = all_n[genre]\n",
    "    \n",
    "    genre_data = []\n",
    "    genre_just_content = []\n",
    "    \n",
    "    for movie_id_index in tqdm(range(len(genre_ids[genre]))):\n",
    "        movie_id = genre_ids[genre][movie_id_index]\n",
    "        if movie_id in id_scripts_content:\n",
    "            genre_data.append((movie_id, id_scripts_content[movie_id]))\n",
    "            genre_just_content.append(id_scripts_content[movie_id])\n",
    "        \n",
    "    averaged_array = []\n",
    "\n",
    "    for movie in genre_just_content:\n",
    "        if len(movie) > 0:\n",
    "            averaged_array.append(np.mean(np.asarray(movie), axis=0).tolist())\n",
    "\n",
    "    if len(averaged_array) > N:\n",
    "        gmm = GaussianMixture(n_components=N)\n",
    "        predicted_subclasses = gmm.fit_predict(averaged_array)\n",
    "\n",
    "        movies_with_subclasses = []\n",
    "\n",
    "        for index in range(len(genre_data)-1):\n",
    "            movie = genre_data[index]\n",
    "            doublet = (movie[0], predicted_subclasses[index])\n",
    "            movies_with_subclasses.append(doublet)\n",
    "\n",
    "        for doublet in movies_with_subclasses:\n",
    "            if doublet[0] in genre_dict:\n",
    "                genre_dict[doublet[0]][1][doublet[0]] = doublet[1]\n",
    "    \n",
    "with open(\"id_to_genre_with_subgenres_dict.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(genre_dict, output_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the subgenres all calculated, we adjust the similarity matrix once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_w_matrix_by_subgenre(w_matrix, genre_dict, factor=2):\n",
    "    adjusted_w_matrix = w_matrix.copy()\n",
    "    for index, row in w_matrix.iterrows():\n",
    "        a_genres = genre_dict[int(row['movie_a'])]\n",
    "        b_genres = genre_dict[int(row['movie_b'])]\n",
    "        intersect = set(a_genres[0]).intersection(set(b_genres[0]))\n",
    "        if len(intersect) > 0:\n",
    "            flag = False\n",
    "            for genre in intersect:\n",
    "                if genre in a_genres[1] and genre in b_genres[1]:\n",
    "                    if a_genres[1][genre] == b_genres[1][genre]:\n",
    "                        flag = True\n",
    "            if flag == True:\n",
    "                adjusted_w_matrix.set_value(index, 'weight', row['weight']*factor)\n",
    "            \n",
    "    return adjusted_w_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of  7070\n",
      "200 out of  7070\n",
      "400 out of  7070\n",
      "600 out of  7070\n",
      "800 out of  7070\n",
      "1000 out of  7070\n",
      "1200 out of  7070\n",
      "1400 out of  7070\n",
      "1600 out of  7070\n",
      "1800 out of  7070\n",
      "2000 out of  7070\n",
      "2200 out of  7070\n",
      "2400 out of  7070\n",
      "2600 out of  7070\n",
      "2800 out of  7070\n",
      "3000 out of  7070\n",
      "3200 out of  7070\n",
      "3400 out of  7070\n",
      "3600 out of  7070\n",
      "3800 out of  7070\n",
      "4000 out of  7070\n",
      "4200 out of  7070\n",
      "4400 out of  7070\n",
      "4600 out of  7070\n",
      "4800 out of  7070\n",
      "5000 out of  7070\n",
      "5200 out of  7070\n",
      "5400 out of  7070\n",
      "5600 out of  7070\n",
      "5800 out of  7070\n",
      "6000 out of  7070\n",
      "6200 out of  7070\n",
      "6400 out of  7070\n",
      "6600 out of  7070\n",
      "6800 out of  7070\n",
      "7000 out of  7070\n",
      "Evaluation result - precision: 0.881413, recall: 0.986821\n"
     ]
    }
   ],
   "source": [
    "with open(\"id_to_genre_with_subgenres_dict.pickle\", \"wb\") as output_pickle:\n",
    "    pickle.dump(genre_dict, output_pickle)\n",
    "\n",
    "w_matrix_subgenres = adjust_w_matrix_by_subgenre(w_matrix_genres, genre_dict, 1.1)\n",
    "\n",
    "# run the evaluation\n",
    "eval_result = binary_eval(ratings_test, w_matrix_subgenres, adjusted_ratings, rating_mean)\n",
    "print('Evaluation result - precision: %f, recall: %f' % eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
